{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRNN은 기존의 뉴럴넷과 다른점이 있다면, 이전의 output이 다음의 cell로 연결된다는 것.\\n텐서플로우를 사용하면 쉽게 구현가능\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "RNN은 기존의 뉴럴넷과 다른점이 있다면, 이전의 output이 다음의 cell로 연결된다는 것.\n",
    "텐서플로우를 사용하면 쉽게 구현가능\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "# shape=(1,1,4) # input shape [[[1,0,0,0]]]\n",
    "hidden_size=2 # hidden size를 무엇으로 정했는지에따라 출력이 결정된다.\n",
    "# input data의 dimension이 뭐든 상관없이 출력의 값을 2개로 주겠다..라는 것\n",
    "# -> output shape=(1,1,2)   [[[x,x]]]\n",
    "# 이런건 cell을 만들때 정의하는 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstep 2\\n그럼 이 cell을 만든 것을 구동을 시켜서 입력을 주고 출력을 뽑아야하는데, \\n이것을 보통 드라이버라고 부를 수도 있고, 다른 형태의 함수; wrapper라고 볼수 있는데 보통\\ntf.nn.dynamic_rnn을 쓰게 된다.\\n여기에 만든 cell을 넘겨주고 우리가 원하는 input을 넘겨주게 된다.\\n그럼 두가지 출력을 내는데,\\n하나는 output, 또하나는 마지막 state의 값을 준다.\\nstate는 우리가 잘 사용할 일이 없다\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "step 1\n",
    "cell을 만들어야 한다. 뭐 어떤 형태의 cell을 만들것인가? 뭐  basic, lstm, gru\n",
    "cell을 만들때 가장 중요한 것은 \n",
    "cell에서 나가는 output의 size가 얼마일까(hidden_size 혹은 num_units라고 함)\n",
    "\"\"\"\n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "\"\"\"\n",
    "step 2\n",
    "그럼 이 cell을 만든 것을 구동을 시켜서 입력을 주고 출력을 뽑아야하는데, \n",
    "이것을 보통 드라이버라고 부를 수도 있고, 다른 형태의 함수; wrapper라고 볼수 있는데 보통\n",
    "tf.nn.dynamic_rnn을 쓰게 된다.\n",
    "여기에 만든 cell을 넘겨주고 우리가 원하는 input을 넘겨주게 된다.\n",
    "그럼 두가지 출력을 내는데,\n",
    "하나는 output, 또하나는 마지막 state의 값을 준다.\n",
    "state는 우리가 잘 사용할 일이 없다\n",
    "\"\"\"\n",
    "# outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.]]], dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable one_cell/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\n  File \"/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 179, in __call__\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n  File \"<ipython-input-20-afa2c6d20c79>\", line 13, in <module>\n    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-afa2c6d20c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    710\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2626\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2627\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2457\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2459\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2460\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2461\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2407\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2409\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    695\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    696\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_or_size_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m       \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0;31m# i = input_gate, j = new_input, f = forget_gate, o = output_gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_start, scope)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mouter_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     weights = vs.get_variable(\n\u001b[0;32m--> 747\u001b[0;31m         \"weights\", [total_arg_size, output_size], dtype=dtype)\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable one_cell/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\n  File \"/Users/junkyu/.pyenv/versions/3.5.2/envs/tensorflow-env-3.5.2/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 179, in __call__\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n  File \"<ipython-input-20-afa2c6d20c79>\", line 13, in <module>\n    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cell생성 부분과 cell을 통해 학습하고 구동하는 부분을 나누어 줌으로써 \n",
    "원하는 형태의 cell을 맘대로 바꿔줄 수가 있다.\n",
    "\"\"\"\n",
    "\n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "with tf.variable_scope('one_cell') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size)\n",
    "\n",
    "    x_data = np.array([[[1,0,0,0]]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "(1, 5, 4)\n",
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]]], dtype=float32)\n",
      "array([[[ 0.15969434, -0.33226603],\n",
      "        [ 0.22477764,  0.26550105],\n",
      "        [ 0.09447759, -0.33812353],\n",
      "        [-0.45583069, -0.68295962],\n",
      "        [-0.43204474,  0.28075731]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "shape=(1,5,4)\n",
    "sequence_length: series data를 몇개를 받을 것인가에 대한 값; 한번에 cell을 몇번 펼칠 것인가\n",
    "이건 입력데이터를 줄때 그 입력 어레이의 형태에 의해 알아서 결정된다.\n",
    "\"\"\"\n",
    "# One hot encoding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "with tf.variable_scope('two_sequences') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    print(cell.output_size, cell.state_size)\n",
    "    \n",
    "    # 아래와 같이 sequence size를 만들어준다. seq = 5\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())\n",
    "    # outputs들은 각 cell의 weight들일 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[-0.10906062,  0.05750896],\n",
      "        [ 0.00738194, -0.085712  ],\n",
      "        [ 0.15759379,  0.08211433],\n",
      "        [ 0.28534856,  0.18914004],\n",
      "        [ 0.41674593,  0.08413108]],\n",
      "\n",
      "       [[ 0.09222777, -0.10889792],\n",
      "        [ 0.18850979, -0.04409115],\n",
      "        [ 0.27115873,  0.10533011],\n",
      "        [ 0.36266097,  0.2067491 ],\n",
      "        [ 0.4219299 ,  0.26672372]],\n",
      "\n",
      "       [[ 0.15875447,  0.12790295],\n",
      "        [ 0.28807002,  0.21608226],\n",
      "        [ 0.35791621,  0.0764155 ],\n",
      "        [ 0.37852645, -0.04007913],\n",
      "        [ 0.3931796 ,  0.12647699]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# batching input\n",
    "\"\"\"\n",
    "batch size를 통해 여러데이터를 줄 수 있음.\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('3_batches') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(\n",
    "        cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[-0.00645624,  0.18257737],\n",
      "        [ 0.091427  ,  0.07146088],\n",
      "        [ 0.00766226,  0.10999069],\n",
      "        [-0.10878152,  0.12820807],\n",
      "        [-0.03603111, -0.00460226]],\n",
      "\n",
      "       [[ 0.08416969, -0.07597371],\n",
      "        [ 0.11446248, -0.13593888],\n",
      "        [-0.03475482, -0.10425442],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.13012083,  0.03296331],\n",
      "        [-0.22653149,  0.04358187],\n",
      "        [-0.01091214, -0.0578985 ],\n",
      "        [ 0.07281873, -0.13319963],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('3_batches_dynamic_length') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(\n",
    "        cell, x_data, sequence_length=[5,3,4], dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[-0.07022887,  0.07325557],\n",
      "        [-0.10870773,  0.04461332],\n",
      "        [-0.21258216,  0.06556352],\n",
      "        [-0.24651808,  0.0781047 ],\n",
      "        [-0.22741725,  0.03849515]],\n",
      "\n",
      "       [[-0.07597632,  0.00397045],\n",
      "        [-0.12754203, -0.02652754],\n",
      "        [-0.19907251,  0.02990212],\n",
      "        [-0.23771062,  0.05684341],\n",
      "        [-0.26317525,  0.07246798]],\n",
      "\n",
      "       [[-0.11238696,  0.04529959],\n",
      "        [-0.18382685,  0.06915338],\n",
      "        [-0.16327067,  0.05812208],\n",
      "        [-0.18752436,  0.04390874],\n",
      "        [-0.28457615,  0.06262349]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('initial_state') as scope:\n",
    "    batch_size = 3\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                      [e, o, l, l, l],\n",
    "                      [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    # One cell RNN input_dim(4) -> output_dim (5). sequence: 5, batch: 3\n",
    "    hidden_size=2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                        initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[  0.,   1.,   2.],\n",
      "        [  3.,   4.,   5.],\n",
      "        [  6.,   7.,   8.],\n",
      "        [  9.,  10.,  11.],\n",
      "        [ 12.,  13.,  14.]],\n",
      "\n",
      "       [[ 15.,  16.,  17.],\n",
      "        [ 18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.],\n",
      "        [ 24.,  25.,  26.],\n",
      "        [ 27.,  28.,  29.]],\n",
      "\n",
      "       [[ 30.,  31.,  32.],\n",
      "        [ 33.,  34.,  35.],\n",
      "        [ 36.,  37.,  38.],\n",
      "        [ 39.,  40.,  41.],\n",
      "        [ 42.,  43.,  44.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create input data\n",
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "pp.pprint(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[  6.15068600e-02,  -7.43666217e-02,   4.65881824e-02,\n",
      "           5.86086847e-02,  -9.43710878e-02],\n",
      "        [  1.02229059e-01,  -8.10933486e-02,   8.56113359e-02,\n",
      "          -5.01152277e-02,  -7.56732225e-02],\n",
      "        [  8.67768228e-02,  -4.30066176e-02,   7.35513419e-02,\n",
      "          -6.15403913e-02,  -1.32910563e-02],\n",
      "        [  6.05235547e-02,  -1.82999838e-02,   4.87128533e-02,\n",
      "          -3.60763706e-02,  -1.38329586e-03],\n",
      "        [  3.95366997e-02,  -6.95752772e-03,   2.82309819e-02,\n",
      "          -1.92505363e-02,  -1.86250967e-04]],\n",
      "\n",
      "       [[  1.80041985e-04,  -2.51258304e-03,   9.76169249e-04,\n",
      "          -9.80630610e-03,  -2.75549337e-05],\n",
      "        [  1.50887732e-04,  -8.76367674e-04,   8.18992732e-04,\n",
      "          -4.97637223e-03,  -4.11127485e-06],\n",
      "        [  1.02296261e-04,  -3.02609318e-04,   5.18253539e-04,\n",
      "          -2.48353858e-03,  -6.05849834e-07],\n",
      "        [  6.56756529e-05,  -1.03831517e-04,   2.95369449e-04,\n",
      "          -1.22612470e-03,  -8.90204319e-08],\n",
      "        [  4.14452952e-05,  -3.55121920e-05,   1.60026728e-04,\n",
      "          -6.00615575e-04,  -1.30514870e-08]],\n",
      "\n",
      "       [[  5.54119985e-08,  -1.21266567e-05,   3.17132617e-06,\n",
      "          -2.92547513e-04,  -1.91054306e-09],\n",
      "        [  4.55827482e-08,  -4.13656562e-06,   2.60709817e-06,\n",
      "          -1.41882134e-04,  -2.79467088e-10],\n",
      "        [  3.06439993e-08,  -1.41056148e-06,   1.63613049e-06,\n",
      "          -6.85938503e-05,  -4.08499450e-11],\n",
      "        [  1.95719725e-08,  -4.80877929e-07,   9.28389170e-07,\n",
      "          -3.30819348e-05,  -5.96891980e-12],\n",
      "        [  1.23106858e-08,  -1.63914919e-07,   5.01796592e-07,\n",
      "          -1.59251140e-05,  -8.71976536e-13]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('generated_data') as scope:\n",
    "    # One cell RNN input_dum (3) -> output_dim (5). sequence: 5, batch: 3\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                        initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.596759\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Cost: sequence_loss \n",
    "그 후에 output이 얼마나 좋은가를 loss를 통해 계산을 해야함. 그래야 학습이 가능하지\n",
    "나온 값의 sequence를 다 알고 있으니 각각을 뭐 cross_entropy같은 걸로 계산해서 평균을 낸다던지 해서\n",
    "할 수 있긴하다. 근데 복잡해질 수 있으니\n",
    "tensorflow에서 sequence_loss라는 것이 있음.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim ]\n",
    "\n",
    "# onehot으로 나타냄(softmax랑 상당히 유사)\n",
    "prediction = tf.constant([[[0.2, 0.7], [0.6, 0.2], [0.2, 0.9]]], dtype=tf.float32) \n",
    "\n",
    "# [batch_size * sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "\"\"\"\n",
    "logits: 우리의 예측\n",
    "targets: true data\n",
    "weights: 각각의 자리를 얼마나 중요하게 생각하느냐.. 그냥 모두다 1이라 생각\n",
    "\"\"\"\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets=y_data, weights=weights)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Loss: ', sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1:  0.513015 Loss2:  0.371101\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim]\n",
    "prediction1 = tf.constant([[[0.3, 0.7], [0.3, 0.7], [0.3, 0.7]]],\n",
    "                         dtype=tf.float32)\n",
    "prediction2 = tf.constant([[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]],\n",
    "                         dtype=tf.float32) # prediction1보다 강하게 1로 예측\n",
    "\n",
    "# [batch_size * sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(prediction1, y_data, weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(prediction2, y_data, weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Loss1: \", sequence_loss1.eval(),\n",
    "     \"Loss2: \", sequence_loss2.eval())\n",
    "\"\"\"\n",
    "원하는 값이 실제에 가까워질수록 loss가 1에 비해 낮아진다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
