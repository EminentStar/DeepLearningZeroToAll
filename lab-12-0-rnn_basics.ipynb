{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRNN은 기존의 뉴럴넷과 다른점이 있다면, 이전의 output이 다음의 cell로 연결된다는 것.\\n텐서플로우를 사용하면 쉽게 구현가능\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "RNN은 기존의 뉴럴넷과 다른점이 있다면, 이전의 output이 다음의 cell로 연결된다는 것.\n",
    "텐서플로우를 사용하면 쉽게 구현가능\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "# shape=(1,1,4) # input shape [[[1,0,0,0]]]\n",
    "hidden_size=2 # hidden size를 무엇으로 정했는지에따라 출력이 결정된다.\n",
    "# input data의 dimension이 뭐든 상관없이 출력의 값을 2개로 주겠다..라는 것\n",
    "# -> output shape=(1,1,2)   [[[x,x]]]\n",
    "# 이런건 cell을 만들때 정의하는 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstep 2\\n그럼 이 cell을 만든 것을 구동을 시켜서 입력을 주고 출력을 뽑아야하는데, \\n이것을 보통 드라이버라고 부를 수도 있고, 다른 형태의 함수; wrapper라고 볼수 있는데 보통\\ntf.nn.dynamic_rnn을 쓰게 된다.\\n여기에 만든 cell을 넘겨주고 우리가 원하는 input을 넘겨주게 된다.\\n그럼 두가지 출력을 내는데,\\n하나는 output, 또하나는 마지막 state의 값을 준다.\\nstate는 우리가 잘 사용할 일이 없다\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "step 1\n",
    "cell을 만들어야 한다. 뭐 어떤 형태의 cell을 만들것인가? 뭐  basic, lstm, gru\n",
    "cell을 만들때 가장 중요한 것은 \n",
    "cell에서 나가는 output의 size가 얼마일까(hidden_size 혹은 num_units라고 함)\n",
    "\"\"\"\n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "\"\"\"\n",
    "step 2\n",
    "그럼 이 cell을 만든 것을 구동을 시켜서 입력을 주고 출력을 뽑아야하는데, \n",
    "이것을 보통 드라이버라고 부를 수도 있고, 다른 형태의 함수; wrapper라고 볼수 있는데 보통\n",
    "tf.nn.dynamic_rnn을 쓰게 된다.\n",
    "여기에 만든 cell을 넘겨주고 우리가 원하는 input을 넘겨주게 된다.\n",
    "그럼 두가지 출력을 내는데,\n",
    "하나는 output, 또하나는 마지막 state의 값을 준다.\n",
    "state는 우리가 잘 사용할 일이 없다\n",
    "\"\"\"\n",
    "# outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.]]], dtype=float32)\n",
      "array([[[ 0.06986494, -0.02265078]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cell생성 부분과 cell을 통해 학습하고 구동하는 부분을 나누어 줌으로써 \n",
    "원하는 형태의 cell을 맘대로 바꿔줄 수가 있다.\n",
    "\"\"\"\n",
    "\n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "with tf.variable_scope('one_cell') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size)\n",
    "\n",
    "    x_data = np.array([[[1,0,0,0]]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "(1, 5, 4)\n",
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]]], dtype=float32)\n",
      "array([[[ 0.17770417,  0.68937284],\n",
      "        [ 0.06786316,  0.28095865],\n",
      "        [ 0.47286847,  0.1634223 ],\n",
      "        [ 0.59231973,  0.11166678],\n",
      "        [-0.41298565,  0.4807488 ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "shape=(1,5,4)\n",
    "sequence_length: series data를 몇개를 받을 것인가에 대한 값; 한번에 cell을 몇번 펼칠 것인가\n",
    "이건 입력데이터를 줄때 그 입력 어레이의 형태에 의해 알아서 결정된다.\n",
    "\"\"\"\n",
    "# One hot encoding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "with tf.variable_scope('two_sequences') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    print(cell.output_size, cell.state_size)\n",
    "    \n",
    "    # 아래와 같이 sequence size를 만들어준다. seq = 5\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())\n",
    "    # outputs들은 각 cell의 weight들일 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[ 0.12355028,  0.14976273],\n",
      "        [ 0.03199994,  0.13515949],\n",
      "        [ 0.21534435,  0.19047116],\n",
      "        [ 0.3102963 ,  0.22424267],\n",
      "        [ 0.12143589,  0.09742174]],\n",
      "\n",
      "       [[-0.04306879,  0.02006755],\n",
      "        [-0.19016083, -0.12135446],\n",
      "        [ 0.03798136, -0.03051605],\n",
      "        [ 0.2019273 ,  0.04879034],\n",
      "        [ 0.30317193,  0.12097759]],\n",
      "\n",
      "       [[ 0.17746802,  0.0670521 ],\n",
      "        [ 0.28810859,  0.13191102],\n",
      "        [ 0.09361219,  0.11960442],\n",
      "        [ 0.03508233,  0.1101391 ],\n",
      "        [ 0.2188635 ,  0.1688305 ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# batching input\n",
    "\"\"\"\n",
    "batch size를 통해 여러데이터를 줄 수 있음.\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('3_batches') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(\n",
    "        cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[-0.02298108,  0.03827505],\n",
      "        [-0.01660769,  0.0673784 ],\n",
      "        [-0.01992308,  0.09115271],\n",
      "        [-0.02127427,  0.12768787],\n",
      "        [-0.01037671, -0.03410594]],\n",
      "\n",
      "       [[-0.00406266,  0.03409239],\n",
      "        [ 0.01195425, -0.14932822],\n",
      "        [ 0.01309387, -0.03287434],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.00216312,  0.0592601 ],\n",
      "        [-0.00722144,  0.1042966 ],\n",
      "        [-0.01352181,  0.16157055],\n",
      "        [-0.02126524,  0.15845685],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('3_batches_dynamic_length') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(\n",
    "        cell, x_data, sequence_length=[5,3,4], dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[-0.01479058,  0.08751657],\n",
      "        [ 0.10128039,  0.09110713],\n",
      "        [ 0.17514136, -0.01490271],\n",
      "        [ 0.20155185, -0.12458292],\n",
      "        [ 0.1491058 ,  0.05986462]],\n",
      "\n",
      "       [[ 0.10309401,  0.05207301],\n",
      "        [ 0.07751615,  0.17546369],\n",
      "        [ 0.17291632,  0.03411548],\n",
      "        [ 0.20253746, -0.09338085],\n",
      "        [ 0.21093722, -0.17432097]],\n",
      "\n",
      "       [[ 0.11367605, -0.11007044],\n",
      "        [ 0.16244458, -0.18185474],\n",
      "        [ 0.18534388, -0.02177148],\n",
      "        [ 0.22163045,  0.02945917],\n",
      "        [ 0.23751295, -0.08408798]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('initial_state') as scope:\n",
    "    batch_size = 3\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                      [e, o, l, l, l],\n",
    "                      [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    # One cell RNN input_dim(4) -> output_dim (5). sequence: 5, batch: 3\n",
    "    hidden_size=2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                        initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[  0.,   1.,   2.],\n",
      "        [  3.,   4.,   5.],\n",
      "        [  6.,   7.,   8.],\n",
      "        [  9.,  10.,  11.],\n",
      "        [ 12.,  13.,  14.]],\n",
      "\n",
      "       [[ 15.,  16.,  17.],\n",
      "        [ 18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.],\n",
      "        [ 24.,  25.,  26.],\n",
      "        [ 27.,  28.,  29.]],\n",
      "\n",
      "       [[ 30.,  31.,  32.],\n",
      "        [ 33.,  34.,  35.],\n",
      "        [ 36.,  37.,  38.],\n",
      "        [ 39.,  40.,  41.],\n",
      "        [ 42.,  43.,  44.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create input data\n",
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "pp.pprint(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ -1.07050009e-01,   6.00064099e-02,  -8.09521377e-02,\n",
      "          -1.15258425e-01,  -1.10853501e-01],\n",
      "        [ -2.76925325e-01,   1.89776607e-02,  -5.93931638e-02,\n",
      "           3.67132165e-02,  -3.82090837e-01],\n",
      "        [ -2.37706795e-01,   2.61245295e-03,  -1.20805688e-02,\n",
      "           2.57515103e-01,  -6.06447041e-01],\n",
      "        [ -1.41193748e-01,   3.16908292e-04,  -1.91839423e-03,\n",
      "           3.97887677e-01,  -7.47776091e-01],\n",
      "        [ -7.55769834e-02,   3.76467797e-05,  -2.96965009e-04,\n",
      "           4.76116806e-01,  -8.41632843e-01]],\n",
      "\n",
      "       [[ -3.79775055e-02,   5.67152483e-06,  -4.18035634e-05,\n",
      "           5.33180654e-01,  -4.20296967e-01],\n",
      "        [ -2.21759137e-02,   6.10068639e-07,  -8.26958694e-06,\n",
      "           5.57956219e-01,  -7.07988918e-01],\n",
      "        [ -1.06543126e-02,   6.54783960e-08,  -1.25419785e-06,\n",
      "           5.79231739e-01,  -8.60566199e-01],\n",
      "        [ -5.16101578e-03,   7.34453609e-09,  -1.93413769e-07,\n",
      "           5.98074496e-01,  -9.36282694e-01],\n",
      "        [ -2.54468201e-03,   8.44300074e-10,  -3.04939398e-08,\n",
      "           6.15196109e-01,  -9.72158492e-01]],\n",
      "\n",
      "       [[ -1.30203983e-03,   1.28311972e-10,  -4.54128823e-09,\n",
      "           6.35580063e-01,  -4.66984034e-01],\n",
      "        [ -7.29016552e-04,   1.37668826e-11,  -8.93680019e-10,\n",
      "           6.49190128e-01,  -7.75961101e-01],\n",
      "        [ -3.42056446e-04,   1.45915322e-12,  -1.34814965e-10,\n",
      "           6.59724772e-01,  -9.14407432e-01],\n",
      "        [ -1.65203455e-04,   1.63860353e-13,  -2.08991609e-11,\n",
      "           6.70455635e-01,  -9.68975484e-01],\n",
      "        [ -8.17322580e-05,   1.89349167e-14,  -3.31685439e-12,\n",
      "           6.80663288e-01,  -9.89165604e-01]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('generated_data') as scope:\n",
    "    # One cell RNN input_dum (3) -> output_dim (5). sequence: 5, batch: 3\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                        initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.596759\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Cost: sequence_loss \n",
    "그 후에 output이 얼마나 좋은가를 loss를 통해 계산을 해야함. 그래야 학습이 가능하지\n",
    "나온 값의 sequence를 다 알고 있으니 각각을 뭐 cross_entropy같은 걸로 계산해서 평균을 낸다던지 해서\n",
    "할 수 있긴하다. 근데 복잡해질 수 있으니\n",
    "tensorflow에서 sequence_loss라는 것이 있음.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim ]\n",
    "\n",
    "# onehot으로 나타냄(softmax랑 상당히 유사)\n",
    "prediction = tf.constant([[[0.2, 0.7], [0.6, 0.2], [0.2, 0.9]]], dtype=tf.float32) \n",
    "\n",
    "# [batch_size * sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "\"\"\"\n",
    "logits: 우리의 예측\n",
    "targets: true data\n",
    "weights: 각각의 자리를 얼마나 중요하게 생각하느냐.. 그냥 모두다 1이라 생각\n",
    "\"\"\"\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets=y_data, weights=weights)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Loss: ', sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1:  0.513015 Loss2:  0.371101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n원하는 값이 실제에 가까워질수록 loss가 1에 비해 낮아진다.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim]\n",
    "prediction1 = tf.constant([[[0.3, 0.7], [0.3, 0.7], [0.3, 0.7]]],\n",
    "                         dtype=tf.float32)\n",
    "prediction2 = tf.constant([[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]],\n",
    "                         dtype=tf.float32) # prediction1보다 강하게 1로 예측\n",
    "\n",
    "# [batch_size * sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(prediction1, y_data, weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(prediction2, y_data, weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Loss1: \", sequence_loss1.eval(),\n",
    "     \"Loss2: \", sequence_loss2.eval())\n",
    "\"\"\"\n",
    "원하는 값이 실제에 가까워질수록 loss가 1에 비해 낮아진다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[  0.,   1.,   2.],\n",
      "        [  3.,   4.,   5.],\n",
      "        [  6.,   7.,   8.],\n",
      "        [  9.,  10.,  11.],\n",
      "        [ 12.,  13.,  14.]],\n",
      "\n",
      "       [[ 15.,  16.,  17.],\n",
      "        [ 18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.],\n",
      "        [ 24.,  25.,  26.],\n",
      "        [ 27.,  28.,  29.]],\n",
      "\n",
      "       [[ 30.,  31.,  32.],\n",
      "        [ 33.,  34.,  35.],\n",
      "        [ 36.,  37.,  38.],\n",
      "        [ 39.,  40.,  41.],\n",
      "        [ 42.,  43.,  44.]]], dtype=float32)\n",
      "dynamic rnn:  Tensor(\"MultiRNNCell/rnn/transpose:0\", shape=(3, 5, 5), dtype=float32)\n",
      "array([[[  2.27656172e-04,   2.16634973e-04,   1.47411658e-04,\n",
      "           1.17256393e-04,   1.59535746e-04],\n",
      "        [  9.91133973e-04,   1.02063862e-03,   6.32673269e-04,\n",
      "           5.82273933e-04,   6.01870124e-04],\n",
      "        [  2.58713146e-03,   2.33225478e-03,   1.51518453e-03,\n",
      "           1.50144601e-03,   1.55601243e-03],\n",
      "        [  5.12063829e-03,   3.75815760e-03,   2.66761472e-03,\n",
      "           2.85365968e-03,   3.16592283e-03],\n",
      "        [  8.50854255e-03,   4.85967053e-03,   3.87415616e-03,\n",
      "           4.54929844e-03,   5.46113728e-03]],\n",
      "\n",
      "       [[  1.73808599e-03,  -1.07092061e-03,   4.25082224e-04,\n",
      "          -1.79608251e-04,   2.15895171e-03],\n",
      "        [  5.82361780e-03,  -3.71371629e-03,   1.04801240e-03,\n",
      "          -3.69545160e-04,   7.03925639e-03],\n",
      "        [  1.18926866e-02,  -7.85532221e-03,   1.47206243e-03,\n",
      "          -3.36461817e-04,   1.40703367e-02],\n",
      "        [  1.92015432e-02,  -1.31411981e-02,   1.46767928e-03,\n",
      "           4.13350645e-05,   2.23769099e-02],\n",
      "        [  2.70007104e-02,  -1.91318616e-02,   9.99260345e-04,\n",
      "           7.60853232e-04,   3.11623085e-02]],\n",
      "\n",
      "       [[  1.87378202e-03,  -1.17924216e-03,   4.49754501e-04,\n",
      "          -2.02382478e-04,   2.33341358e-03],\n",
      "        [  6.17996836e-03,  -4.03084885e-03,   1.07512239e-03,\n",
      "          -4.20026685e-04,   7.49091338e-03],\n",
      "        [  1.24868322e-02,  -8.44065100e-03,   1.46362022e-03,\n",
      "          -4.10759792e-04,   1.48223443e-02],\n",
      "        [  2.00071298e-02,  -1.40137523e-02,   1.39497453e-03,\n",
      "          -5.29347235e-05,   2.34066863e-02],\n",
      "        [  2.79727187e-02,  -2.02799533e-02,   8.49625969e-04,\n",
      "           6.47520414e-04,   3.24268006e-02]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create input data\n",
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "pp.pprint(x_data)  # batch, sequence_length, input_dim\n",
    "\n",
    "with tf.variable_scope('MultiRNNCell') as scope:\n",
    "    # Make rnn\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    cell = rnn.MultiRNNCell([cell] * 4, state_is_tuple=True) # 3 layer\n",
    "    \n",
    "    # rnn in/out\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    print('dynamic rnn: ', outputs)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic rnn:  Tensor(\"dynamic_rnn/rnn/transpose:0\", shape=(3, 5, 5), dtype=float32)\n",
      "array([[[ -6.57164529e-02,  -8.03739876e-02,   2.22690985e-01,\n",
      "           1.24483649e-02,  -1.84512585e-01],\n",
      "        [ -4.68223952e-02,   2.09278211e-01,   4.46212053e-01,\n",
      "           1.48645230e-03,  -2.55340040e-01],\n",
      "        [ -1.97796002e-02,   2.19276577e-01,   5.38733900e-01,\n",
      "           3.40516526e-05,  -1.27545893e-01],\n",
      "        [ -5.53835649e-03,   1.89141750e-01,   5.62418818e-01,\n",
      "           4.34651838e-07,  -3.80422734e-02],\n",
      "        [ -1.04363670e-03,   1.64377004e-01,   5.55978000e-01,\n",
      "           4.57295801e-09,  -1.14335306e-02]],\n",
      "\n",
      "       [[  8.71740049e-05,   1.56013623e-01,   3.68248997e-03,\n",
      "           3.52323795e-11,  -3.38438665e-03],\n",
      "        [  3.38054815e-05,   1.29815146e-01,   4.62183775e-03,\n",
      "           4.34483477e-13,  -1.13829586e-03],\n",
      "        [  8.62929755e-06,   1.11826949e-01,   4.79643745e-03,\n",
      "           4.69142284e-15,  -3.82726983e-04],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]],\n",
      "\n",
      "       [[  4.21430038e-08,   6.96936548e-02,   9.18966816e-06,\n",
      "           5.93044878e-21,  -1.46216380e-05],\n",
      "        [  1.00036042e-08,   5.71740344e-02,   1.18123753e-05,\n",
      "           6.71070404e-23,  -4.87933357e-06],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dynamic RNN의 장점은 확실하게 없는 데이터를 0으로 만들어줘서 loss가 잘 작동하도록 한다.\n",
    "sequence_length를 주기만 하면 된다.\n",
    "\"\"\"\n",
    "with tf.variable_scope('dynamic_rnn') as scope:\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32,\n",
    "                                        sequence_length=[5, 3, 2]) # sequence_length를 임의로 줘보자\n",
    "    # length 1 for batch 1, length 2 for batch 2\n",
    "    \n",
    "    print('dynamic rnn: ', outputs)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
