# Lecture 8-2
### Back-propagation and appearance of 2006/2007 'Deep'


* 2006/2007 논문
  - 각각의 w값에 처음에 초기값을 잘주기만 한다면 많은 layer의 뉴럴넷도 잘 학습할 수 있다.
  - 깊은 layer를 구축하면 굉장히 복잡한 문제도 풀 수 있다는 것을 증명
  - 이 이후로 사람들이 다시 관심을 가지게 됨
  - 그래서 이때 사람들이 Neural Networks라고 하면 처다보지도 않으니깐, 이름을 좀 바꿔서 Deep Nets, Deep Learning으로 바꾸자. 라고 하게 됨(Rebranding)

* 정말 주목을 받게 된 계기가 Image net 대회. Imagenet Classification에서 에러율을 확 줄임(이제 거의 3%까지 내림)

* 이제는 그림을 설명하는 수준까지 옴.
